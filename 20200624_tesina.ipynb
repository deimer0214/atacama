{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, ngrams\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Se carga e ¿l archivo de stop words\n",
    "def stopwords (tmp_entrada):\n",
    "    stop = open(\"C:/Users/deimer0214/PycharmProjects/tatacoa/stopwords.txt\", \"r\",  encoding=\"utf8\")\n",
    "    lista_stop=[]\n",
    "    for st in stop:\n",
    "        lista_tmp=[]\n",
    "        st=st[0:st[0].find('\\n')]\n",
    "        lista_stop.append(st)\n",
    "    for tr in lista_stop:\n",
    "        tmp_entrada = [x for x in tmp_entrada if x != tr]\n",
    "    return tmp_entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Se carga lematización para pasar todo a presente \n",
    "def CargarDiccionarioLemas():\n",
    "    file = open(\"C:/Users/deimer0214/PycharmProjects/tatacoa/diccionarioLematizador.txt\", \"rb\")\n",
    "    lema_d={}\n",
    "    for line in file:\n",
    "        bloques = line.split()\n",
    "        palabra = bloques[0]\n",
    "        lema = bloques[1]\n",
    "        lema_d.update({palabra:lema})\n",
    "    return lema_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pdte\n",
    "def lematizador(lema_d,palabra):\n",
    "    palabra=palabra.lower()\n",
    "    if palabra in lema_d:\n",
    "        lema = str(lema_d.get(palabra))\n",
    "    else:\n",
    "        lema = palabra\n",
    "    return lema\n",
    "\n",
    "lema_d = CargarDiccionarioLemas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Carga de Json que sale de tweepy\n",
    "with open('C:/Users/deimer0214/PycharmProjects/tatacoa/cien_tweets.json') as f:\n",
    "    data = json.load(f)\n",
    "    lista_registros = []\n",
    "    for l in data[\"Name\"]:\n",
    "        tmpCampo=l['text']\n",
    "        if (len(tmpCampo))>0:\n",
    "            terminos = {}\n",
    "            lineaSplit = word_tokenize(tmpCampo.lower())\n",
    "            for palabra in lineaSplit:\n",
    "                terminos[palabra] = 1\n",
    "            tmp_salida = stopwords(terminos)\n",
    "            lista_palabras = []\n",
    "            for palabra in tmp_salida:\n",
    "                lista_palabras.append(lematizador(lema_d, palabra))\n",
    "            #lista_registros = {\"Vector\": lista_palabras}\n",
    "            lista_registros.append(lista_palabras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aca en adelante procurar hacerlo bien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp_ms</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1567481491044</td>\n",
       "      <td>RT @ShefVaidya: Will all the Indian Americans ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1567481492155</td>\n",
       "      <td>RT @TomFitton: Obama used Comey to spy on @Rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1567481493104</td>\n",
       "      <td>RT @JudicialWatch: Hillary Clinton is not abov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1567481493810</td>\n",
       "      <td>RT @dmills3710: Devlin Barrett, As Reporter, @...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1567481495499</td>\n",
       "      <td>RT @charliekirk11: RT if\\n\\nJames Comey\\n\\nJoh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    timestamp_ms                                               text\n",
       "0  1567481491044  RT @ShefVaidya: Will all the Indian Americans ...\n",
       "1  1567481492155  RT @TomFitton: Obama used Comey to spy on @Rea...\n",
       "2  1567481493104  RT @JudicialWatch: Hillary Clinton is not abov...\n",
       "3  1567481493810  RT @dmills3710: Devlin Barrett, As Reporter, @...\n",
       "4  1567481495499  RT @charliekirk11: RT if\\n\\nJames Comey\\n\\nJoh..."
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data['Name'][42]['text']\n",
    "df_in = pd.DataFrame(data['Name'])\n",
    "data2=df_in[['timestamp_ms','text']]\n",
    "data2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(txt):\n",
    "    txt=\"\".join([c for c in txt if c not in string.puntuation])\n",
    "    tokens = re.split('\\w+', txt)\n",
    "    txt=[ps.stem(word) for word in tokens if wordnot in stopwords]\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this': 6, 'is': 3, 'sentence': 4, 'another': 0, 'third': 5, 'document': 1, 'here': 2}\n",
      "['another', 'document', 'here', 'is', 'sentence', 'third', 'this']\n",
      "(3, 7)\n",
      "  (0, 6)\t0.4760629392767929\n",
      "  (0, 4)\t0.4760629392767929\n",
      "  (0, 3)\t0.7394106813498714\n",
      "  (1, 6)\t0.4804583972923858\n",
      "  (1, 4)\t0.4804583972923858\n",
      "  (1, 3)\t0.3731188059313277\n",
      "  (1, 0)\t0.6317450542765208\n",
      "  (2, 5)\t0.546454011634009\n",
      "  (2, 3)\t0.3227445421804912\n",
      "  (2, 2)\t0.546454011634009\n",
      "  (2, 1)\t0.546454011634009\n",
      "[[0.         0.         0.         0.73941068 0.47606294 0.\n",
      "  0.47606294]\n",
      " [0.63174505 0.         0.         0.37311881 0.4804584  0.\n",
      "  0.4804584 ]\n",
      " [0.         0.54645401 0.54645401 0.32274454 0.         0.54645401\n",
      "  0.        ]]\n",
      "    another  document      here        is  sentence     third      this\n",
      "0  0.000000  0.000000  0.000000  0.739411  0.476063  0.000000  0.476063\n",
      "1  0.631745  0.000000  0.000000  0.373119  0.480458  0.000000  0.480458\n",
      "2  0.000000  0.546454  0.546454  0.322745  0.000000  0.546454  0.000000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vect = TfidfVectorizer()\n",
    "\n",
    "corpus = [\"This is a sentence is\",\n",
    "          \"This is another sentence\",\n",
    "          \"Third document is here\"]\n",
    "\n",
    "X = tfidf_vect.fit(corpus)\n",
    "print(X.vocabulary_)\n",
    "print(tfidf_vect.get_feature_names())\n",
    "\n",
    "X = tfidf_vect.transform(corpus)\n",
    "print(X.shape)\n",
    "print(X)\n",
    "print(X.toarray())\n",
    "\n",
    "df = pd.DataFrame(X.toarray(), columns = tfidf_vect.get_feature_names())\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
